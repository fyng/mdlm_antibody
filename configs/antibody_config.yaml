mode: train  # train / ppl_eval / sample_eval
diffusion: absorbing_state
backbone: dit  # dit / dimamba / ar
parameterization: subs  # subs / d3pm / sedd
time_conditioning: False
T: 0  # 0 (continuous time) / 1000 
subs_masking: False

seed: 1

noise:
  type: loglinear
  sigma_min: 1e-4
  sigma_max: 20

# Training configuration
training:
  mode: train  # 'train' | 'sample'
  epochs: 5
  num_workers: 2
  ema: 0 # disable weights EMA
  sampling_eps: 1e-3
  change_of_variables: False
  batch_size: 256

eval:
  checkpoint_path: ''  # Used to evaluate a checkpoint after training.
  disable_ema: False
  compute_generative_perplexity: False
  perplexity_batch_size: 8
  compute_perplexity_on_sanity: False
  gen_ppl_eval_model_name_or_path: gpt2-large  # gpt2-large, meta-llama/Llama-2-7b-hf
  generate_samples: True
  
# Optimizer settings
optimizer:
  lr: 1e-3
  warmup_steps: 5000  # Number of steps for linear warmup
  betas: [0.9, 0.95]
  weight_decay: 0.01
  grad_clip: 1.0
  t_max: 1000         # for cosine annealing scheduler

# Model architecture (DDIT small)
model:
  hidden_size: 768
  cond_dim: 128
  length: 8
  n_blocks: 12
  n_heads: 12
  scale_by_sigma: true
  dropout: 0.1
  tie_word_embeddings: false
  max_len: 384  # 280 for imgt, 299 for aho

# Sampling settings
sampling:
  num_steps: 128
  store_traj: false
  predictor: ddpm_cache  # analytic, ddpm, ddpm_cache

